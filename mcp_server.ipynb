{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3521234f",
   "metadata": {},
   "source": [
    "# MCP Server with LangGraph Documentation\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to build AI agents using **LangGraph** with tool calling capabilities, progressing from synchronous to asynchronous workflows, and finally integrating with **Model Context Protocol (MCP)** servers.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Main Topics Covered](#main-topics-covered)\n",
    "3. [Prerequisites](#prerequisites)\n",
    "4. [Section 1: Synchronous LangGraph with Custom Tools](#section-1-synchronous-langgraph-with-custom-tools)\n",
    "5. [Section 2: Asynchronous Workflow](#section-2-asynchronous-workflow)\n",
    "6. [Section 3: MCP Server Integration](#section-3-mcp-server-integration)\n",
    "7. [Key Learnings](#key-learnings)\n",
    "8. [Common Issues and Solutions](#common-issues-and-solutions)\n",
    "9. [Best Practices](#best-practices)\n",
    "10. [Next Steps](#next-steps)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook explores building intelligent conversational agents that can use tools to perform tasks. It demonstrates three progressively advanced implementations:\n",
    "\n",
    "1. **Basic synchronous tool-calling agent** with custom tools\n",
    "2. **Asynchronous agent** for better performance\n",
    "3. **MCP-integrated agent** that can connect to multiple servers (local and remote)\n",
    "\n",
    "---\n",
    "\n",
    "## Main Topics Covered\n",
    "\n",
    "### 1. LangGraph Framework\n",
    "- Building stateful conversation graphs\n",
    "- Managing message history with `add_messages`\n",
    "- Defining nodes and edges in a graph\n",
    "- Conditional routing based on tool calls\n",
    "\n",
    "### 2. Tool Integration\n",
    "- Creating custom tools with `@tool` decorator\n",
    "- Binding tools to LLMs\n",
    "- Using `ToolNode` for automatic tool execution\n",
    "- Handling tool responses\n",
    "\n",
    "### 3. State Management\n",
    "- Defining `ChatState` with TypedDict\n",
    "- Using `Annotated` types for state reduction\n",
    "- Message accumulation patterns\n",
    "\n",
    "### 4. Asynchronous Programming\n",
    "- Converting synchronous code to async\n",
    "- Using `await` vs `asyncio.run()` in Jupyter notebooks\n",
    "- Async invocation patterns with LangGraph\n",
    "\n",
    "### 5. Model Context Protocol (MCP)\n",
    "- Connecting to local MCP servers (stdio transport)\n",
    "- Connecting to remote MCP servers (HTTP/SSE transport)\n",
    "- Using `MultiServerMCPClient` to manage multiple servers\n",
    "- Adapting MCP tools for LangChain compatibility\n",
    "\n",
    "### 6. LLM Provider Integration\n",
    "- Using Groq's ChatGroq for fast inference\n",
    "- Handling API-specific requirements (tool message formatting)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Required Libraries\n",
    "```python\n",
    "langgraph\n",
    "langchain-core\n",
    "langchain-groq\n",
    "langchain-community\n",
    "langchain-mcp-adapters\n",
    "python-dotenv\n",
    "fastmcp\n",
    "```\n",
    "\n",
    "### Environment Setup\n",
    "- Python 3.13+\n",
    "- Virtual environment configured\n",
    "- `.env` file with API keys (GROQ_API_KEY, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Synchronous LangGraph with Custom Tools\n",
    "\n",
    "### What's Demonstrated\n",
    "This section shows how to build a basic LangGraph agent with a custom calculator tool.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### 1. Tool Definition\n",
    "```python\n",
    "@tool\n",
    "def calculator(first_num: float, second_num: float, operation: str) -> dict:\n",
    "    \"\"\"\n",
    "    Perform a basic arithmetic operation on two numbers.\n",
    "    Supported operations: add, sub, mul, div\n",
    "    \"\"\"\n",
    "    # Implementation handles add, sub, mul, div operations\n",
    "```\n",
    "\n",
    "**Learning Point**: Tools must have clear docstrings - the LLM uses these to decide when to call the tool.\n",
    "\n",
    "#### 2. LLM Configuration\n",
    "```python\n",
    "llm = ChatGroq(model='openai/gpt-oss-20b')\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "```\n",
    "\n",
    "**Learning Point**: `bind_tools()` gives the LLM awareness of available tools without requiring manual prompting.\n",
    "\n",
    "#### 3. State Definition\n",
    "```python\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "```\n",
    "\n",
    "**Learning Point**: `add_messages` is a reducer function that automatically handles message accumulation, preventing duplicates and maintaining conversation history.\n",
    "\n",
    "#### 4. Graph Construction\n",
    "```python\n",
    "graph = StateGraph(ChatState)\n",
    "graph.add_node(\"chat_node\", chat_node)\n",
    "graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph.add_edge(START, \"chat_node\")\n",
    "graph.add_conditional_edges(\"chat_node\", tools_condition)\n",
    "graph.add_edge(\"tools\", \"chat_node\")\n",
    "```\n",
    "\n",
    "**Learning Point**: \n",
    "- `tools_condition` automatically determines if the LLM wants to call a tool\n",
    "- The graph creates a loop: chat â†’ tools â†’ chat, allowing multiple tool calls\n",
    "- This enables the agent to use tools iteratively to solve complex problems\n",
    "\n",
    "#### 5. Execution\n",
    "```python\n",
    "chatbot = graph.compile()\n",
    "result = chatbot.invoke({\"messages\": [HumanMessage(content=\"...\")]})\n",
    "```\n",
    "\n",
    "### What You Learned\n",
    "- âœ… How to create custom LangChain tools\n",
    "- âœ… Building a stateful conversation graph\n",
    "- âœ… Synchronous tool calling with automatic routing\n",
    "- âœ… Message state management\n",
    "\n",
    "---\n",
    "\n",
    "## Section 2: Asynchronous Workflow\n",
    "\n",
    "### Why Async?\n",
    "Asynchronous programming allows:\n",
    "- Better performance for I/O-bound operations (API calls)\n",
    "- Non-blocking execution\n",
    "- Concurrent operations\n",
    "- Improved scalability\n",
    "\n",
    "### Key Changes from Sync Version\n",
    "\n",
    "#### 1. Async Node Definition\n",
    "```python\n",
    "async def chat_node(state: ChatState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = await llm_with_tools.ainvoke(messages)  # Note: ainvoke\n",
    "    return {'messages': [response]}\n",
    "```\n",
    "\n",
    "**Learning Point**: Use `ainvoke()` instead of `invoke()` for async operations.\n",
    "\n",
    "#### 2. Async Graph Building\n",
    "```python\n",
    "async def main():\n",
    "    chatbot = build_graph()\n",
    "    result = await chatbot.ainvoke({\"messages\": [...]})\n",
    "```\n",
    "\n",
    "#### 3. Jupyter-Specific Execution\n",
    "```python\n",
    "# âŒ DON'T use asyncio.run() in Jupyter\n",
    "# asyncio.run(main())\n",
    "\n",
    "# âœ… DO use await directly\n",
    "await main()\n",
    "```\n",
    "\n",
    "**Critical Learning**: Jupyter notebooks already run in an event loop. Using `asyncio.run()` will cause:\n",
    "```\n",
    "RuntimeError: asyncio.run() cannot be called from a running event loop\n",
    "```\n",
    "\n",
    "### What You Learned\n",
    "- âœ… Converting synchronous LangGraph to async\n",
    "- âœ… Proper async/await patterns in Jupyter\n",
    "- âœ… Difference between `invoke()` and `ainvoke()`\n",
    "- âœ… Event loop management in notebooks\n",
    "\n",
    "---\n",
    "\n",
    "## Section 3: MCP Server Integration\n",
    "\n",
    "### What is MCP?\n",
    "**Model Context Protocol** is a standardized way to expose tools, resources, and prompts to LLMs. It allows:\n",
    "- Connecting LLMs to external data sources\n",
    "- Exposing APIs as tools\n",
    "- Building modular, reusable tool servers\n",
    "- Combining multiple tool providers\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚          LangGraph Agent                     â”‚\n",
    "â”‚  (ChatGroq + MultiServerMCPClient)           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚                    â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Local MCP     â”‚  â”‚  Remote MCP       â”‚\n",
    "â”‚  Server        â”‚  â”‚  Server           â”‚\n",
    "â”‚  (stdio)       â”‚  â”‚  (HTTP/SSE)       â”‚\n",
    "â”‚                â”‚  â”‚                   â”‚\n",
    "â”‚  - add()       â”‚  â”‚  - add_expense()  â”‚\n",
    "â”‚  - multiply()  â”‚  â”‚  - get_expenses() â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### MCP Client Configuration\n",
    "\n",
    "```python\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"arith\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"/path/to/python\",\n",
    "            \"args\": [\"/path/to/main.py\"],\n",
    "        },\n",
    "        \"expense\": {\n",
    "            \"transport\": \"streamable_http\",\n",
    "            \"url\": \"https://example.fastmcp.app/mcp\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "**Learning Points**:\n",
    "- **stdio transport**: Launches a local process, communicates via stdin/stdout\n",
    "- **streamable_http transport**: Connects to HTTP-based MCP servers\n",
    "- Use full path to Python executable to ensure correct virtual environment\n",
    "- Each server gets a unique identifier (\"arith\", \"expense\")\n",
    "\n",
    "### Dynamic Tool Loading\n",
    "\n",
    "```python\n",
    "async def build_graph():\n",
    "    tools = await client.get_tools()  # Fetches tools from all servers\n",
    "    print(f\"Loaded tools: {[t.name for t in tools]}\")\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "```\n",
    "\n",
    "**Learning Point**: Tools are fetched dynamically at runtime, allowing flexible integration without hardcoding tool definitions.\n",
    "\n",
    "### Groq API Compatibility Fix\n",
    "\n",
    "```python\n",
    "async def chat_node(state: ChatState):\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Fix tool message format for Groq\n",
    "    for msg in messages:\n",
    "        if msg.type == \"tool\" and not isinstance(msg.content, str):\n",
    "            msg.content = json.dumps(msg.content)\n",
    "    \n",
    "    response = await llm_with_tools.ainvoke(messages)\n",
    "    return {'messages': [response]}\n",
    "```\n",
    "\n",
    "**Critical Learning**: Different LLM providers have different requirements:\n",
    "- **Groq** requires tool messages to have string content\n",
    "- **MCP tools** often return structured objects or content arrays\n",
    "- Solution: Convert complex tool outputs to JSON strings\n",
    "\n",
    "### What You Learned\n",
    "- âœ… Model Context Protocol fundamentals\n",
    "- âœ… Connecting to local MCP servers (stdio)\n",
    "- âœ… Connecting to remote MCP servers (HTTP)\n",
    "- âœ… Managing multiple MCP servers simultaneously\n",
    "- âœ… Handling LLM provider-specific formatting requirements\n",
    "- âœ… Dynamic tool discovery and integration\n",
    "\n",
    "---\n",
    "\n",
    "## Key Learnings\n",
    "\n",
    "### 1. **Graph-Based Agent Architecture**\n",
    "LangGraph provides a powerful abstraction for building agents:\n",
    "- **Nodes** represent operations (LLM calls, tool execution)\n",
    "- **Edges** define control flow\n",
    "- **Conditional edges** enable dynamic routing\n",
    "- **State** maintains conversation context\n",
    "\n",
    "### 2. **Tool Calling Patterns**\n",
    "```\n",
    "User Message â†’ LLM (decides to use tool) â†’ Tool Execution â†’ LLM (uses result) â†’ Final Response\n",
    "```\n",
    "This pattern can repeat multiple times in one conversation turn.\n",
    "\n",
    "### 3. **Async is Essential for Production**\n",
    "- Enables concurrent tool calls\n",
    "- Better resource utilization\n",
    "- Improved response times\n",
    "- Scales better under load\n",
    "\n",
    "### 4. **MCP Enables Modularity**\n",
    "- Separate tool logic from agent logic\n",
    "- Tools can be written in any language\n",
    "- Easy to swap or upgrade tool providers\n",
    "- Share tools across different agents/applications\n",
    "\n",
    "### 5. **Provider-Specific Quirks**\n",
    "Different LLM providers have different:\n",
    "- Message format requirements\n",
    "- Tool calling conventions\n",
    "- Rate limits and performance characteristics\n",
    "\n",
    "Always test with your specific provider and handle edge cases.\n",
    "\n",
    "---\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "### Issue 1: `asyncio.run()` Error in Jupyter\n",
    "**Error**: `RuntimeError: asyncio.run() cannot be called from a running event loop`\n",
    "\n",
    "**Solution**: Use `await` directly instead of `asyncio.run()`\n",
    "```python\n",
    "# âŒ Wrong\n",
    "asyncio.run(main())\n",
    "\n",
    "# âœ… Correct\n",
    "await main()\n",
    "```\n",
    "\n",
    "### Issue 2: Tool Message Format Error with Groq\n",
    "**Error**: `BadRequestError: 'messages.2.content' : value must be a string`\n",
    "\n",
    "**Solution**: Convert tool outputs to strings\n",
    "```python\n",
    "for msg in messages:\n",
    "    if msg.type == \"tool\" and not isinstance(msg.content, str):\n",
    "        msg.content = json.dumps(msg.content)\n",
    "```\n",
    "\n",
    "### Issue 3: MCP Server Not Found\n",
    "**Error**: Server fails to start or connect\n",
    "\n",
    "**Solutions**:\n",
    "- Use absolute path to Python interpreter\n",
    "- Verify virtual environment is activated\n",
    "- Check that FastMCP server file exists\n",
    "- Ensure all dependencies are installed in the correct environment\n",
    "\n",
    "### Issue 4: Tools Not Being Called\n",
    "**Possible Causes**:\n",
    "- Tool description is unclear to the LLM\n",
    "- Tool parameters don't match user request\n",
    "- LLM decides the task doesn't require tools\n",
    "\n",
    "**Solutions**:\n",
    "- Write clear, specific tool docstrings\n",
    "- Test with explicit requests (\"use the calculator tool...\")\n",
    "- Check if LLM has general knowledge of the task\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### 1. Tool Design\n",
    "- âœ… Write clear, concise tool descriptions\n",
    "- âœ… Use type hints for all parameters\n",
    "- âœ… Return consistent data structures\n",
    "- âœ… Handle errors gracefully within tools\n",
    "- âœ… Document supported operations/formats\n",
    "\n",
    "### 2. State Management\n",
    "- âœ… Use `add_messages` for automatic message accumulation\n",
    "- âœ… Keep state minimal - only store what's needed\n",
    "- âœ… Use TypedDict for type safety\n",
    "\n",
    "### 3. Error Handling\n",
    "- âœ… Wrap tool execution in try-except blocks\n",
    "- âœ… Return informative error messages\n",
    "- âœ… Log errors for debugging\n",
    "- âœ… Provide fallback behavior when tools fail\n",
    "\n",
    "### 4. Performance\n",
    "- âœ… Use async for I/O-bound operations\n",
    "- âœ… Consider caching tool results when appropriate\n",
    "- âœ… Set reasonable timeouts for tool execution\n",
    "- âœ… Monitor API rate limits\n",
    "\n",
    "### 5. MCP Integration\n",
    "- âœ… Use separate MCP servers for different domains\n",
    "- âœ… Version your MCP servers\n",
    "- âœ… Document tool schemas clearly\n",
    "- âœ… Test MCP servers independently before integration\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Immediate Enhancements\n",
    "1. **Add more tools** to your MCP servers\n",
    "2. **Implement streaming** for real-time responses\n",
    "3. **Add memory/persistence** to maintain conversation history across sessions\n",
    "4. **Error recovery** - handle tool failures gracefully\n",
    "\n",
    "### Advanced Topics to Explore\n",
    "1. **LangGraph Checkpointing** - Save and resume agent state\n",
    "2. **Human-in-the-loop** - Add approval steps for sensitive operations\n",
    "3. **Multi-agent systems** - Coordinate multiple specialized agents\n",
    "4. **Custom routing logic** - Build sophisticated decision trees\n",
    "5. **Tool result validation** - Verify tool outputs before using them\n",
    "6. **Parallel tool execution** - Run independent tools concurrently\n",
    "\n",
    "### Production Considerations\n",
    "1. **Authentication & Authorization** for MCP servers\n",
    "2. **Rate limiting** and quota management\n",
    "3. **Monitoring & observability** - Track tool usage, latency, errors\n",
    "4. **Testing** - Unit tests for tools, integration tests for graphs\n",
    "5. **Deployment** - Containerization, scaling strategies\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "### Documentation\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangChain Tools](https://python.langchain.com/docs/modules/agents/tools/)\n",
    "- [Model Context Protocol Spec](https://modelcontextprotocol.io/)\n",
    "- [FastMCP Library](https://github.com/jlowin/fastmcp)\n",
    "\n",
    "### Related Notebooks in This Repository\n",
    "- `simple_graph_bot.ipynb` - Basic graph concepts\n",
    "- `sequential_flow.ipynb` - Sequential agent workflows\n",
    "- `parallel_workflow.ipynb` - Parallel execution patterns\n",
    "- `iterative_workflow.ipynb` - Iterative problem solving\n",
    "- `persistence.ipynb` - State persistence\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated the evolution from simple tool-calling agents to sophisticated MCP-integrated systems:\n",
    "\n",
    "1. **Started with basics**: Custom tools + LangGraph\n",
    "2. **Added performance**: Async workflows\n",
    "3. **Achieved modularity**: MCP server integration\n",
    "\n",
    "The final implementation shows a production-ready pattern for building agents that can:\n",
    "- Use multiple tool sources\n",
    "- Work with any MCP-compatible server\n",
    "- Handle complex, multi-step tasks\n",
    "- Scale efficiently with async execution\n",
    "\n",
    "The MCP integration pattern is particularly powerful because it allows you to:\n",
    "- Build tools once, use everywhere\n",
    "- Share tools across teams and projects\n",
    "- Integrate third-party tool providers\n",
    "- Keep agent logic separate from tool implementation\n",
    "\n",
    "This architecture is ready for real-world applications! ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a13252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a35591af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,START\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_classic.tools import DuckDuckGoSearchRun\n",
    "from langgraph.graph.message import add_messages,BaseMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_classic.tools import tool\n",
    "from typing import Annotated,TypedDict\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "load_dotenv()\n",
    "llm= ChatGroq(model='openai/gpt-oss-20b',)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42f408f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculator(first_num: float, second_num: float, operation: str) -> dict:\n",
    "    \"\"\"\n",
    "    Perform a basic arithmetic operation on two numbers.\n",
    "    Supported operations: add, sub, mul, div\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if operation == \"add\":\n",
    "            result = first_num + second_num\n",
    "        elif operation == \"sub\":\n",
    "            result = first_num - second_num\n",
    "        elif operation == \"mul\":\n",
    "            result = first_num * second_num\n",
    "        elif operation == \"div\":\n",
    "            if second_num == 0:\n",
    "                return {\"error\": \"Division by zero is not allowed\"}\n",
    "            result = first_num / second_num\n",
    "        else:\n",
    "            return {\"error\": f\"Unsupported operation '{operation}'\"}\n",
    "        \n",
    "        return {\"first_num\": first_num, \"second_num\": second_num, \"operation\": operation, \"result\": result}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8b2aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[calculator]\n",
    "llm_with_tools=llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b25acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatState(TypedDict):\n",
    "    messages:Annotated[list[BaseMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5f1ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state=ChatState):\n",
    "    messages=state[\"messages\"]\n",
    "    response=llm_with_tools.invoke(messages)\n",
    "    return {\"messages\":[response]}\n",
    "\n",
    "tool_node=ToolNode(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee6b16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x137b2b230>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph=StateGraph(ChatState)\n",
    "graph.add_node(\"chat_node\", chat_node)\n",
    "graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "# defining graph connections\n",
    "graph.add_edge(START, \"chat_node\")\n",
    "graph.add_conditional_edges(\"chat_node\", tools_condition)\n",
    "graph.add_edge(\"tools\", \"chat_node\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b31555bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x137cf51d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = graph.compile()\n",
    "chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f92d41d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ™ï¸ **Cricket Commentary Voice** ðŸŽ™ï¸\n",
      "\n",
      "\"All right, folks, weâ€™re at the finish line of this thrilling mathematical innings! The batsman, 132,354, has faced a daunting bowling attack of 23 overs. The bowlerâ€™s delivery? A perfect 23. And what a run! The batsman scores a massive 132,354 runs, but the scoreboardâ€™s not finished yet. \n",
      "\n",
      "Now, the crowd is on the edge of their seats as we calculate the final tally. 23 goes into 132,354 exactly 5,754 times, but thereâ€™s a tiny bit of runs left on the field. After all the overs are bowled, the final count on the scoreboard shows a **remainder of 12 runs**. \n",
      "\n",
      "So there you have it, a crisp 12 runs left on the board, and thatâ€™s the final score of this mathematical match! ðŸâœ¨\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# running the graph\n",
    "result = chatbot.invoke({\"messages\": [HumanMessage(content=\"Find the modulus of 132354 and 23 and give answer like a cricket commentator.\")]})\n",
    "\n",
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffb345cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Find the modulus of 132354 and 23 and give answer like a cricket commentator.', additional_kwargs={}, response_metadata={}, id='65a6bf30-2178-402c-9577-39ea5fb9c0b0'), AIMessage(content='ðŸŽ™ï¸ **Cricket Commentary Voice** ðŸŽ™ï¸\\n\\n\"All right, folks, weâ€™re at the finish line of this thrilling mathematical innings! The batsman, 132,354, has faced a daunting bowling attack of 23 overs. The bowlerâ€™s delivery? A perfect 23. And what a run! The batsman scores a massive 132,354 runs, but the scoreboardâ€™s not finished yet. \\n\\nNow, the crowd is on the edge of their seats as we calculate the final tally. 23 goes into 132,354 exactly 5,754 times, but thereâ€™s a tiny bit of runs left on the field. After all the overs are bowled, the final count on the scoreboard shows a **remainder of 12 runs**. \\n\\nSo there you have it, a crisp 12 runs left on the board, and thatâ€™s the final score of this mathematical match! ðŸâœ¨', additional_kwargs={'reasoning_content': 'We need to find modulus (remainder) of 132354 divided by 23. Compute 132354 mod 23. Let\\'s calculate. 23 * 5750 = 132250. 132354 - 132250 = 104. 23 * 4 = 92. 104 - 92 = 12. So remainder 12. Check if any higher multiples: 23*5754=23*5750+23*4=132250+92=132342. 132354-132342=12. Yes remainder 12. So answer: 12. But need to give answer like a cricket commentator. So something like \"And there\\'s a spectacular knock as 132,354 runs are bowled over 23 overs, and the wicket falls with a remainder of 12 runs...\". Let\\'s craft a commentary style.'}, response_metadata={'token_usage': {'completion_tokens': 376, 'prompt_tokens': 160, 'total_tokens': 536, 'completion_time': 0.423186575, 'completion_tokens_details': {'reasoning_tokens': 176}, 'prompt_time': 0.009205532, 'prompt_tokens_details': None, 'queue_time': 0.119815627, 'total_time': 0.432392107}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7a98-af65-7750-8dcd-99fa6263ab9a-0', usage_metadata={'input_tokens': 160, 'output_tokens': 376, 'total_tokens': 536, 'output_token_details': {'reasoning': 176}})]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a27a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b4cbea8",
   "metadata": {},
   "source": [
    "### same code in async  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd576702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Commentator**: *\"And here we go, folks! The ballâ€™s rolling down the pitch, the batâ€™s swinging, the crowdâ€™s roaring. Weâ€™re heading into the final over of this thrilling mathematical match. The bowlerâ€”well, our trusty calculatorâ€”delivers the ball: 132,354 is the batâ€™s number, and 23 is the wicket keeperâ€™s number. The ball arcs, the fielders spread out, and the ballâ€”ah! It hits the boundary! The wicket keeper dives, but the ball slips through. The batsmanâ€”our number 132,354â€”runs, and the fielding sideâ€”23â€”tries to take the catch. Theyâ€™re out! The final count: the remainder is 12. Yes, 12! And thatâ€™s the wicket! A spectacular finish, and the modulus of 132,354 by 23 is a crisp 12. What a delivery!\"}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph,START\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_classic.tools import DuckDuckGoSearchRun\n",
    "from langgraph.graph.message import add_messages,BaseMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_classic.tools import tool\n",
    "from typing import Annotated,TypedDict\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "llm= ChatGroq(model='openai/gpt-oss-20b',)\n",
    "\n",
    "@tool\n",
    "def calculator(first_num: float, second_num: float, operation: str) -> dict:\n",
    "    \"\"\"\n",
    "    Perform a basic arithmetic operation on two numbers.\n",
    "    Supported operations: add, sub, mul, div\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if operation == \"add\":\n",
    "            result = first_num + second_num\n",
    "        elif operation == \"sub\":\n",
    "            result = first_num - second_num\n",
    "        elif operation == \"mul\":\n",
    "            result = first_num * second_num\n",
    "        elif operation == \"div\":\n",
    "            if second_num == 0:\n",
    "                return {\"error\": \"Division by zero is not allowed\"}\n",
    "            result = first_num / second_num\n",
    "        else:\n",
    "            return {\"error\": f\"Unsupported operation '{operation}'\"}\n",
    "        \n",
    "        return {\"first_num\": first_num, \"second_num\": second_num, \"operation\": operation, \"result\": result}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "tools = [calculator]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# state\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def build_graph():\n",
    "\n",
    "    # nodes\n",
    "    async def chat_node(state: ChatState):\n",
    "\n",
    "        messages = state[\"messages\"]\n",
    "        response = await llm_with_tools.ainvoke(messages)\n",
    "        return {'messages': [response]}\n",
    "\n",
    "    tool_node = ToolNode(tools)\n",
    "\n",
    "    # defining graph and nodes\n",
    "    graph = StateGraph(ChatState)\n",
    "\n",
    "    graph.add_node(\"chat_node\", chat_node)\n",
    "    graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "    # defining graph connections\n",
    "    graph.add_edge(START, \"chat_node\")\n",
    "    graph.add_conditional_edges(\"chat_node\", tools_condition)\n",
    "    graph.add_edge(\"tools\", \"chat_node\")\n",
    "\n",
    "    chatbot = graph.compile()\n",
    "\n",
    "    return chatbot\n",
    "\n",
    "async def main():\n",
    "\n",
    "    chatbot = build_graph()\n",
    "\n",
    "    # running the graph\n",
    "    result = await chatbot.ainvoke({\"messages\": [HumanMessage(content=\"Find the modulus of 132354 and 23 and give answer like a cricket commentator.\")]})\n",
    "\n",
    "    print(result['messages'][-1].content)\n",
    "\n",
    "# In Jupyter notebooks, the event loop is already running.\n",
    "# Useing  await main() instead of asyncio.run(main())\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d45d038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tools: ['add', 'multiply', 'add_expense', 'list_expenses', 'summarize']\n",
      "It looks like there are no expense entries recorded for Novemberâ€¯2024 (fromâ€¯2024â€‘11â€‘01 toâ€¯2024â€‘11â€‘30). If you need to add any, just let me know and I can help you log them!\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph,START\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langgraph.graph.message import add_messages,BaseMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from typing import Annotated,TypedDict\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "llm= ChatGroq(model='openai/gpt-oss-20b',)\n",
    "\n",
    "# MCP client for local FastMCP server\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"arith\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"/Users/owner/Desktop/Agentic-AI/graphvenv/bin/python\",          \n",
    "            \"args\": [\"/Users/owner/Desktop/Agentic-AI/main.py\"],\n",
    "        },\n",
    "        \"expense\": {\n",
    "            \"transport\": \"streamable_http\",  # if this fails, try \"sse\"\n",
    "            \"url\": \"https://splendid-gold-dingo.fastmcp.app/mcp\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# state\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "async def build_graph():\n",
    "\n",
    "    tools = await client.get_tools()\n",
    "\n",
    "    print(f\"Loaded tools: {[t.name for t in tools]}\")\n",
    "\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    # nodes\n",
    "    async def chat_node(state: ChatState):\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "        # Groq is strict about tool message content. \n",
    "        # MCP tools often return a list of content blocks, which Groq may reject.\n",
    "        # We ensure all tool messages have string content.\n",
    "        for msg in messages:\n",
    "            if msg.type == \"tool\" and not isinstance(msg.content, str):\n",
    "                msg.content = json.dumps(msg.content)\n",
    "        \n",
    "        response = await llm_with_tools.ainvoke(messages)\n",
    "        return {'messages': [response]}\n",
    "\n",
    "    tool_node = ToolNode(tools)\n",
    "\n",
    "    # defining graph and nodes\n",
    "    graph = StateGraph(ChatState)\n",
    "\n",
    "    graph.add_node(\"chat_node\", chat_node)\n",
    "    graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "    # defining graph connections\n",
    "    graph.add_edge(START, \"chat_node\")\n",
    "    graph.add_conditional_edges(\"chat_node\", tools_condition)\n",
    "    graph.add_edge(\"tools\", \"chat_node\")\n",
    "\n",
    "    chatbot = graph.compile()\n",
    "\n",
    "    return chatbot\n",
    "\n",
    "async def main():\n",
    "\n",
    "    chatbot = await build_graph()\n",
    "\n",
    "    # running the graph\n",
    "    result = await chatbot.ainvoke({\"messages\": [HumanMessage(content=\"Give me all my expenses for the month of Nov from 1 Nov to 30 Nov\")]})\n",
    "\n",
    "    print(result['messages'][-1].content)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4867f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
